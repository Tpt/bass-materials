{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bass training code\n",
    "==================\n",
    "\n",
    "Depends on Python 3, Tensorflow 2.0+ and the HuggingFace tokenizers library.\n",
    "\n",
    "To download the dependencies `pip install tensorflow tokenizers`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import *\n",
    "from glob import glob\n",
    "import gzip\n",
    "import re\n",
    "import numpy as np\n",
    "import random\n",
    "import csv\n",
    "import json\n",
    "from itertools import chain\n",
    "from collections import defaultdict\n",
    "import gc\n",
    "import os\n",
    "from tokenizers import BertWordPieceTokenizer\n",
    "from http import HTTPStatus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we load the constraints data\n",
    "constraints_def = {}\n",
    "\n",
    "mapping_to_wikidata = {\n",
    "    'property id': '^<http://www.wikidata.org/entity/P2302>',\n",
    "    ' constraint type id': '<http://www.wikidata.org/entity/P2302>',\n",
    "    'regex': '<http://www.wikidata.org/entity/P1793>',\n",
    "    'exceptions': '<http://www.wikidata.org/entity/P2303>',\n",
    "    'group by': '<http://www.wikidata.org/entity/P2304>',\n",
    "    'items': '<http://www.wikidata.org/entity/P2305>',\n",
    "    'property': '<http://www.wikidata.org/entity/P2306>',\n",
    "    'namespace': '<http://www.wikidata.org/entity/P2307>',\n",
    "    'class': '<http://www.wikidata.org/entity/P2308>',\n",
    "    'relation': '<http://www.wikidata.org/entity/P2309>',\n",
    "    'minimal date': '<http://www.wikidata.org/entity/P2310>',\n",
    "    'maximum date': '<http://www.wikidata.org/entity/P2311>',\n",
    "    'maximum value': '<http://www.wikidata.org/entity/P2312>',\n",
    "    'minimal value': '<http://www.wikidata.org/entity/P2313>',\n",
    "    'status': '<http://www.wikidata.org/entity/P2316>',\n",
    "    'separator': '<http://www.wikidata.org/entity/P4155>',\n",
    "    'scope': '<http://www.wikidata.org/entity/P4680>'\n",
    "}\n",
    "with open('constraints.tsv', newline='') as fp:\n",
    "    for row in csv.DictReader(fp, dialect='excel-tab'):\n",
    "        predicates = []\n",
    "        objects = []\n",
    "        for k,vs in row.items():\n",
    "            if k != 'constraint id':\n",
    "                for v in vs.split(' '):\n",
    "                    v = v.strip()\n",
    "                    if v:\n",
    "                        predicates.append(mapping_to_wikidata[k])\n",
    "                        objects.append(v)\n",
    "        constraints_def[row['constraint id']] = {'predicates': predicates, 'objects': objects}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we load and preprocess the data. We encode string with integers to allow the data to fit easily into the main memory\n",
    "\n",
    "class GlobalIntEncoder:\n",
    "    def __init__(self):\n",
    "        self._encoding = {\n",
    "            '': 0\n",
    "        }\n",
    "\n",
    "    def encode(self, value: str):\n",
    "        if value is None:\n",
    "            value = ''\n",
    "        value = str(value)\n",
    "        if value not in self._encoding:\n",
    "            self._encoding[value] = len(self._encoding)\n",
    "        return self._encoding[value]\n",
    "\n",
    "    def save(self, file: str):\n",
    "        with open(file, 'wt') as fp:\n",
    "            fp.writelines(l + '\\n' for l in self._encoding.keys())\n",
    "\n",
    "encoder = GlobalIntEncoder()\n",
    "\n",
    "_relation_to_predicate = {\n",
    "    encoder.encode('<http://www.wikidata.org/entity/Q21503252>'): [encoder.encode('<http://www.wikidata.org/entity/P31>')],\n",
    "    encoder.encode('<http://www.wikidata.org/entity/Q21514624>'): [encoder.encode('<http://www.wikidata.org/entity/P279>')],\n",
    "    encoder.encode('<http://www.wikidata.org/entity/Q30208840>'): [encoder.encode('<http://www.wikidata.org/entity/P31>'), encoder.encode('<http://www.wikidata.org/entity/P279>')],\n",
    "}\n",
    "\n",
    "def _convert_values(values: str) -> List[str]:\n",
    "    return [v for v in (_convert_value(v.strip()) for v in values.split(' ')) if v]\n",
    "\n",
    "def _convert_value(value: Optional[str], subject: Optional[str] = None, predicate: Optional[str] = None, obj: Optional[str] = None, other_subject: Optional[str] = None, other_predicate: Optional[str] = None, other_object: Optional[str] = None) -> Optional[str]:\n",
    "    if value is None or value == '':\n",
    "        return 0\n",
    "    value = encoder.encode(value.replace('http://www.wikidata.org/prop/direct/', 'http://www.wikidata.org/entity/'))\n",
    "    if value == subject:\n",
    "        return encoder.encode('subject')\n",
    "    elif value == predicate:\n",
    "        return encoder.encode('predicate')\n",
    "    elif value == obj:\n",
    "        return encoder.encode('object')\n",
    "    elif value == other_subject:\n",
    "        return encoder.encode('other_subject')\n",
    "    elif value == other_predicate:\n",
    "        return encoder.encode('other_predicate')\n",
    "    elif value == other_object:\n",
    "        return encoder.encode('other_object')\n",
    "    else:\n",
    "        return value\n",
    "\n",
    "def _read_entity_desc(line: List[str], desc_position: int) -> Dict[str,Any]:\n",
    "    desc = line[desc_position].strip()\n",
    "    result = {\n",
    "            'entity_predicates': [],\n",
    "            'entity_objects': [],\n",
    "            'entity_labels': [],\n",
    "            'http_content': ''\n",
    "    }\n",
    "    if not desc:\n",
    "        return result\n",
    "    try:\n",
    "        desc = json.loads(desc)\n",
    "    except ValueError:\n",
    "        print('Invalid description: {}'.format(desc))\n",
    "        return result\n",
    "    if desc['type'] == 'page':\n",
    "        try:\n",
    "            status = \"<http://www.w3.org/2011/http-statusCodes#{}>\".format(HTTPStatus(desc['statusCode']).phrase.title().replace(' ', '').replace('-', ''))\n",
    "            result['entity_predicates'].append(_convert_value('<http://wikiba.se/history/ontology#pageStatusCode>'))\n",
    "            result['entity_objects'].append(_convert_value(status))\n",
    "        except ValueError as e:\n",
    "            print(e)\n",
    "        result['http_content'] = desc['content']\n",
    "    elif desc['type'] == 'entity':\n",
    "        result['entity_labels'].extend(desc['labels'].values())\n",
    "        for predicate, objects in desc['facts'].items():\n",
    "            for obj in objects:\n",
    "                result['entity_predicates'].append(_convert_value(predicate))\n",
    "                result['entity_objects'].append(_convert_value(obj))\n",
    "    else:\n",
    "        print('Invalid description: {}'.format(result))\n",
    "    return result\n",
    "\n",
    "def load_dataset(file_path, max_size: int = 100000):\n",
    "    dataset = {\n",
    "        'constraint_id': [],\n",
    "        'constraint_predicates': [],\n",
    "        'constraint_objects': [],\n",
    "        'subject': [],\n",
    "        'predicate': [],\n",
    "        'object': [],\n",
    "        'object_text': [],\n",
    "        'other_subject': [],\n",
    "        'other_predicate': [],\n",
    "        'other_object': [],\n",
    "        'other_object_text': [],\n",
    "        'subject_predicates': [],\n",
    "        'subject_objects': [],\n",
    "        'object_predicates': [],\n",
    "        'object_objects': [],\n",
    "        'other_entity_predicates': [],\n",
    "        'other_entity_objects': [],\n",
    "        'add_subject': [],\n",
    "        'add_predicate': [],\n",
    "        'add_object': [],\n",
    "        'del_subject': [],\n",
    "        'del_predicate': [],\n",
    "        'del_object': []\n",
    "    }\n",
    "    with gzip.open(file_path, 'rt') as fp:\n",
    "        for line_i, line in enumerate(fp):\n",
    "            if line_i == max_size:\n",
    "                break\n",
    "\n",
    "            elements = line.split('\\t')\n",
    "            if elements[0] not in constraints_def:\n",
    "                continue\n",
    "\n",
    "            constraint = constraints_def[elements[0]]\n",
    "            subject = _convert_value(elements[2])\n",
    "            predicate = _convert_value(elements[3])\n",
    "            obj = _convert_value(elements[4])\n",
    "            other_subject = _convert_value(elements[5])\n",
    "            other_predicate = _convert_value(elements[6])\n",
    "            other_object = _convert_value(elements[7])\n",
    "            add_subject = None\n",
    "            add_predicate = None\n",
    "            add_object = None\n",
    "            del_subject= None\n",
    "            del_predicate = None\n",
    "            del_object = None\n",
    "            entity = None\n",
    "            i = 12\n",
    "            while i < len(elements):\n",
    "                if elements[i] == '<http://wikiba.se/history/ontology#addition>':\n",
    "                    add_subject = elements[i - 3]\n",
    "                    add_predicate = elements[i - 2]\n",
    "                    add_object = elements[i - 1]\n",
    "                elif elements[i] == '<http://wikiba.se/history/ontology#deletion>':\n",
    "                    del_subject = elements[i - 3]\n",
    "                    del_predicate = elements[i - 2]\n",
    "                    del_object = elements[i - 1]\n",
    "                else:\n",
    "                    print('Unexpected entity: {}'.format(elements[i-3:i+1]))\n",
    "                    continue\n",
    "                i += 4\n",
    "\n",
    "            subject_desc = _read_entity_desc(elements, -3)\n",
    "            object_desc = _read_entity_desc(elements, -2)\n",
    "            other_entity_desc = _read_entity_desc(elements, -1)\n",
    "            if any(label in object_desc['http_content'] for label in subject_desc['entity_labels']):\n",
    "                object_desc['entity_predicates'].append(_convert_value('<http://wikiba.se/history/ontology#pageContainsLabel>'))\n",
    "                object_desc['entity_objects'].append(subject)\n",
    "            if any(label in object_desc['http_content'] for label in other_entity_desc['entity_labels']):\n",
    "                object_desc['entity_predicates'].append(_convert_value('<http://wikiba.se/history/ontology#pageContainsLabel>'))\n",
    "                object_desc['entity_objects'].append(other_subject)\n",
    "            if any(label in other_entity_desc['http_content'] for label in subject_desc['entity_labels']):\n",
    "                other_entity_desc['entity_predicates'].append(_convert_value('<http://wikiba.se/history/ontology#pageContainsLabel>'))\n",
    "                other_entity_desc['entity_objects'].append(subject)\n",
    "            if any(label in other_entity_desc['http_content'] for label in object_desc['entity_labels']):\n",
    "                other_entity_desc['entity_predicates'].append(_convert_value('<http://wikiba.se/history/ontology#pageContainsLabel>'))\n",
    "                other_entity_desc['entity_objects'].append(obj)\n",
    "\n",
    "            dataset['constraint_id'].append(_convert_value(elements[0]))\n",
    "            dataset['constraint_predicates'].append([_convert_value(v) for v in constraint['predicates']])\n",
    "            dataset['constraint_objects'].append([_convert_value(v) for v in constraint['objects']])\n",
    "            dataset['subject'].append(subject)\n",
    "            dataset['predicate'].append(predicate)\n",
    "            if elements[4].startswith('<http://www.wikidata.org/entity/'):\n",
    "                dataset['object'].append(obj)\n",
    "                dataset['object_text'].append('')\n",
    "            else:\n",
    "                dataset['object'].append(0)\n",
    "                dataset['object_text'].append(elements[4].split('^^')[0])\n",
    "            dataset['other_subject'].append(other_subject)\n",
    "            dataset['other_predicate'].append(other_predicate)\n",
    "            if elements[7].startswith('<http://www.wikidata.org/entity/'):\n",
    "                dataset['other_object'].append(other_object)\n",
    "                dataset['other_object_text'].append('')\n",
    "            else:\n",
    "                dataset['other_object'].append(0)\n",
    "                dataset['other_object_text'].append(elements[7].split('^^')[0])\n",
    "            dataset['add_subject'].append(_convert_value(add_subject, subject, predicate, obj, other_subject, other_predicate, other_object))\n",
    "            dataset['add_predicate'].append(_convert_value(add_predicate, subject, predicate, obj, other_subject, other_predicate, other_object))\n",
    "            dataset['add_object'].append(_convert_value(add_object, subject, predicate, obj, other_subject, other_predicate, other_object))\n",
    "            dataset['del_subject'].append(_convert_value(del_subject, subject, predicate, obj, other_subject, other_predicate, other_object))\n",
    "            dataset['del_predicate'].append(_convert_value(del_predicate, subject, predicate, obj, other_subject, other_predicate, other_object))\n",
    "            dataset['del_object'].append(_convert_value(del_object, subject, predicate, obj, other_subject, other_predicate, other_object))\n",
    "            dataset['subject_predicates'].append(subject_desc['entity_predicates'])\n",
    "            dataset['subject_objects'].append(subject_desc['entity_objects'])\n",
    "            dataset['object_predicates'].append(object_desc['entity_predicates'])\n",
    "            dataset['object_objects'].append(object_desc['entity_objects'])\n",
    "            dataset['other_entity_predicates'].append(other_entity_desc['entity_predicates'])\n",
    "            dataset['other_entity_objects'].append(other_entity_desc['entity_objects'])\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading conflictWith train\n",
      "Loading distinct train\n",
      "Loading inverse train\n",
      "Loading itemRequiresStatement train\n",
      "Loading oneOf train\n",
      "Loading single train\n",
      "Loading type train\n",
      "Loading valueRequiresStatement train\n",
      "Loading valueType train\n",
      "Loading conflictWith dev\n",
      "Loading distinct dev\n",
      "Loading inverse dev\n",
      "Loading itemRequiresStatement dev\n",
      "Loading oneOf dev\n",
      "Loading single dev\n",
      "Loading type dev\n",
      "Loading valueRequiresStatement dev\n",
      "Loading valueType dev\n",
      "Loading conflictWith test\n",
      "Loading distinct test\n",
      "Loading inverse test\n",
      "Loading itemRequiresStatement test\n",
      "Loading oneOf test\n",
      "Loading single test\n",
      "Loading type test\n",
      "Loading valueRequiresStatement test\n",
      "Loading valueType test\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def load(kind: str, targets: List[str]):\n",
    "    result = defaultdict(list)\n",
    "    for target in targets:\n",
    "        print('Loading {} {}'.format(target, kind))\n",
    "        for k,v in load_dataset('constraint-corrections/constraint-corrections-' + target + '.tsv.gz.full.' + kind + '.tsv.gz').items():\n",
    "            result[k] += v\n",
    "    gc.collect()\n",
    "    result = {k: np.asarray(v) for k,v in result.items()}\n",
    "    gc.collect()\n",
    "    return result\n",
    "target = '*'\n",
    "if target == '*':\n",
    "    targets = ['conflictWith', 'distinct', 'inverse', 'itemRequiresStatement', 'oneOf', 'single', 'type', 'valueRequiresStatement', 'valueType']\n",
    "\n",
    "    train_dataset = load('train', targets)\n",
    "    dev_dataset = load('dev', targets)\n",
    "    test_dataset = {target: load('test', [target]) for target in targets}\n",
    "else:\n",
    "    train_dataset = load('train', [target])\n",
    "    dev_dataset = load('dev', [target])\n",
    "    test_dataset = {target: load('test', [target])}\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4500 past violations for 649 constraints\n",
      "with subject desc: 0.992 (average length: 16.012096774193548)\n",
      "with object desc: 0.798 (average length: 19.10359231411863)\n",
      "with object web page: 0.1648888888888889 (with label in page: 0.5646900269541779)\n",
      "with other triple: 1449 (0.322)\n",
      "with other entity desc: 0.2768888888888889 (average length: 7.06099518459069)\n",
      "with other entity web page: 0.08355555555555555 (with label in page: 0.5452127659574468)\n",
      "in input: add subject: 1.0 add predicate: 0.278336686787391 add object: 0.18376928236083165 del subject: 1.0 del predicate: 1.0 del object: 0.9901869158878505\n",
      "add: 2982 (0.6626666666666666, subject 1.0 known object 0.18376928236083165 known)\n",
      "del: 2140 (0.47555555555555556, subject 1.0 known object 0.9901869158878505 known)\n"
     ]
    }
   ],
   "source": [
    "# Prints some statistics about the dataset\n",
    "from collections import defaultdict\n",
    "\n",
    "def dataset_stats(dataset: Iterable[dict]):\n",
    "    known_entities = [encoder.encode('subject'), encoder.encode('predicate'), encoder.encode('object'), encoder.encode('other_subject'), encoder.encode('other_predicate'), encoder.encode('other_object'), encoder.encode('constraint_predicate')]\n",
    "    count = 0\n",
    "    constraints = defaultdict(int)\n",
    "    with_subject_desc = 0\n",
    "    subject_desc_sum = 0\n",
    "    with_object_desc = 0\n",
    "    object_desc_sum = 0\n",
    "    with_object_http_status = 0\n",
    "    object_http_contains_label = 0\n",
    "    with_other_triple = 0\n",
    "    with_other_entity_desc = 0\n",
    "    other_entity_desc_sum = 0\n",
    "    with_other_entity_http_status = 0\n",
    "    other_entity_http_contains_label = 0\n",
    "    with_add_subject = 0\n",
    "    with_add_predicate = 0\n",
    "    with_add_object = 0\n",
    "    with_del_subject = 0\n",
    "    with_del_predicate = 0\n",
    "    with_del_object = 0\n",
    "    with_add_subject_in_input = 0\n",
    "    with_add_predicate_in_input = 0\n",
    "    with_add_object_in_input = 0\n",
    "    with_del_subject_in_input = 0\n",
    "    with_del_predicate_in_input = 0\n",
    "    with_del_object_in_input = 0\n",
    "    for i in range(len(dataset['predicate'])):\n",
    "        count += 1\n",
    "        constraints[dataset['constraint_id'][i]] += 1\n",
    "        if dataset['subject_predicates'][i]:\n",
    "            with_subject_desc += 1\n",
    "            subject_desc_sum += len(dataset['subject_predicates'][i])\n",
    "            assert len(dataset['subject_predicates'][i]) == len(dataset['subject_objects'][i]) # and len(dataset['subject_predicates'][i]) == len(dataset['subject_objects_text'][i])\n",
    "        if dataset['object_predicates'][i]:\n",
    "            with_object_desc += 1\n",
    "            object_desc_sum += len(dataset['object_predicates'][i])\n",
    "            assert len(dataset['object_predicates'][i]) == len(dataset['object_objects'][i]) # and len(dataset['object_predicates'][i]) == len(dataset['object_objects_text'][i])\n",
    "        if encoder.encode('<http://wikiba.se/history/ontology#pageStatusCode>') in dataset['object_predicates'][i]:\n",
    "            with_object_http_status += 1\n",
    "        if encoder.encode('<http://wikiba.se/history/ontology#pageContainsLabel>') in dataset['object_predicates'][i]:\n",
    "            object_http_contains_label += 1\n",
    "        if dataset['other_subject'][i]:\n",
    "            with_other_triple += 1\n",
    "        if dataset['other_entity_predicates'][i]:\n",
    "            with_other_entity_desc += 1\n",
    "            other_entity_desc_sum += len(dataset['other_entity_predicates'][i])\n",
    "            assert len(dataset['other_entity_predicates'][i]) == len(dataset['other_entity_objects'][i]) # and len(dataset['other_entity_predicates'][i]) == len(dataset['other_entity_objects_text'][i])\n",
    "        if encoder.encode('<http://wikiba.se/history/ontology#pageStatusCode>') in dataset['other_entity_predicates'][i]:\n",
    "            with_other_entity_http_status += 1\n",
    "        if encoder.encode('<http://wikiba.se/history/ontology#pageContainsLabel>') in dataset['other_entity_predicates'][i]:\n",
    "            other_entity_http_contains_label += 1\n",
    "        if dataset['add_subject'][i]:\n",
    "            with_add_subject += 1\n",
    "            if dataset['add_subject'][i] in known_entities:\n",
    "                with_add_subject_in_input += 1\n",
    "        if dataset['add_predicate'][i]:\n",
    "            with_add_predicate += 1\n",
    "            if dataset['add_predicate'][i] in known_entities:\n",
    "                with_add_predicate_in_input += 1\n",
    "        if dataset['add_object'][i]:\n",
    "            with_add_object += 1\n",
    "            if dataset['add_object'][i] in known_entities:\n",
    "                with_add_object_in_input += 1\n",
    "        if dataset['del_subject'][i]:\n",
    "            with_del_subject += 1\n",
    "            if dataset['del_subject'][i] in known_entities:\n",
    "                with_del_subject_in_input += 1\n",
    "        if dataset['del_predicate'][i]:\n",
    "            with_del_predicate += 1\n",
    "            if dataset['del_predicate'][i] in known_entities:\n",
    "                with_del_predicate_in_input += 1\n",
    "        if dataset['del_object'][i]:\n",
    "            with_del_object += 1\n",
    "            if dataset['del_object'][i] in known_entities:\n",
    "                with_del_object_in_input += 1\n",
    "    print('{} past violations for {} constraints'.format(sum(constraints.values()), len(constraints)))\n",
    "    print('with subject desc: {} (average length: {})'.format(with_subject_desc / count, subject_desc_sum / with_subject_desc))\n",
    "    print('with object desc: {} (average length: {})'.format(with_object_desc / count, object_desc_sum / with_object_desc))\n",
    "    print('with object web page: {} (with label in page: {})'.format(with_object_http_status / count, object_http_contains_label / with_object_http_status if with_object_http_status else '?'))\n",
    "    print('with other triple: {} ({})'.format(with_other_triple, with_other_triple / count))\n",
    "    print('with other entity desc: {} (average length: {})'.format(with_other_entity_desc / count, other_entity_desc_sum / with_other_entity_desc if with_other_entity_desc else '?'))\n",
    "    print('with other entity web page: {} (with label in page: {})'.format(with_other_entity_http_status / count, other_entity_http_contains_label / with_other_entity_http_status if with_other_entity_http_status else '?'))\n",
    "    print('in input: add subject: {} add predicate: {} add object: {} del subject: {} del predicate: {} del object: {}'.format(with_add_subject_in_input / with_add_subject, with_add_predicate_in_input / with_add_predicate, with_add_object_in_input / with_add_object, with_del_subject_in_input / with_del_subject, with_del_predicate_in_input / with_del_predicate, with_del_object_in_input / with_del_object))\n",
    "    print('add: {} ({}, subject {} known object {} known)'.format(with_add_subject, with_add_subject / count, with_add_subject_in_input / with_add_subject if with_add_subject else '?', with_add_object_in_input / with_add_object if with_add_object else '?'))\n",
    "    print('del: {} ({}, subject {} known object {} known)'.format(with_del_subject, with_del_subject / count, with_del_subject_in_input / with_del_subject if with_del_subject else '?', with_del_object_in_input / with_del_object if with_del_object else '?'))\n",
    "dataset_stats(dev_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train the BERT tokenizer\n",
    "with open('raw_corrections_text.txt', 'wt') as fp:\n",
    "    for line in chain(\n",
    "        train_dataset['object_text'],\n",
    "        train_dataset['other_object_text']\n",
    "    ):\n",
    "        if line:\n",
    "            fp.write(line)\n",
    "            fp.write('\\n')\n",
    "tokenizer = BertWordPieceTokenizer()\n",
    "tokenizer.train('raw_corrections_text.txt')\n",
    "os.remove('raw_corrections_text.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final preprocessing and model code\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "SEQUENCE_SIZE = 64\n",
    "tokenizer.enable_padding(max_length=SEQUENCE_SIZE)\n",
    "tokenizer.enable_truncation(max_length=SEQUENCE_SIZE)\n",
    "\n",
    "\n",
    "def tokenize_sequence(sequence: List[str]) -> np.array:\n",
    "    matrix = np.zeros((len(sequence), SEQUENCE_SIZE), dtype='int32')\n",
    "    for i,v in enumerate(tokenizer.encode_batch(list(sequence))):\n",
    "        matrix[i] = v.ids\n",
    "    return matrix\n",
    "\n",
    "class TermEncoder:\n",
    "    def __init__(self,  min_occurences_count, max_sequence_length):\n",
    "        self._terms_index = {0: 0}\n",
    "        self._terms_inverse_index = [0]\n",
    "        self._terms_count = {}\n",
    "        self._min_occurences_count = min_occurences_count\n",
    "        self._max_sequence_length = max_sequence_length\n",
    "\n",
    "    def fit(self, input: str):\n",
    "        for value in input:\n",
    "            if value not in self._terms_index:\n",
    "                c = self._terms_count.get(value, 0) + 1\n",
    "                self._terms_count[value] = c\n",
    "                if c == self._min_occurences_count:\n",
    "                    self._terms_index[value] = len(self._terms_inverse_index)\n",
    "                    self._terms_inverse_index.append(value)\n",
    "                    del self._terms_count[value]\n",
    "    \n",
    "    def fit_sequence(self, sequence: np.array):\n",
    "        for values in sequence:\n",
    "            self.fit(values)\n",
    "            \n",
    "    def transform(self, input: np.array) -> np.array:\n",
    "        return np.array([self._terms_index.get(v, 0) for v in input])\n",
    "\n",
    "    def transform_sequence(self, input: np.array) -> np.array:\n",
    "        return keras.preprocessing.sequence.pad_sequences([[self._terms_index.get(v, 0) for v in values] for values in input], maxlen=self._max_sequence_length)\n",
    "\n",
    "    def decode(self, input: np.array) -> np.array:\n",
    "        return np.array([self._terms_inverse_index[v] for v in input])\n",
    "\n",
    "    def save(self, file: str):\n",
    "        with open(file, 'wt') as fp:\n",
    "            fp.writelines('{}\\n'.format(l) for l in self._terms_inverse_index)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self._terms_inverse_index)\n",
    "\n",
    "class DatasetSequence(keras.utils.Sequence):\n",
    "    def __init__(self, dataset, constraint_id_encoder, predicate_encoder, entity_encoder, output_predicate_encoder, output_entity_encoder, batch_size: int, shuffle: bool = False):\n",
    "        self.dataset = dataset\n",
    "        self._constraint_id_encoder = constraint_id_encoder\n",
    "        self._entity_encoder = entity_encoder\n",
    "        self._predicate_encoder = predicate_encoder\n",
    "        self._output_entity_encoder = output_entity_encoder\n",
    "        self._output_predicate_encoder = output_predicate_encoder\n",
    "        self.batch_size = batch_size\n",
    "        self._shuffle = shuffle\n",
    "        self.indices = np.arange(len(self.dataset['add_subject']))\n",
    "        self.on_epoch_end()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset['add_subject']) // self.batch_size\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        inds = self.indices[idx * self.batch_size:(idx + 1) * self.batch_size]\n",
    "        \n",
    "        return {\n",
    "            'constraint_id': self._constraint_id_encoder.transform(self.dataset['constraint_id'][inds]),\n",
    "            'constraint_predicates': self._predicate_encoder.transform_sequence(self.dataset['constraint_predicates'][inds]),\n",
    "            'constraint_objects': self._predicate_encoder.transform_sequence(self.dataset['constraint_objects'][inds]),\n",
    "            'subject': self._entity_encoder.transform(self.dataset['subject'][inds]),\n",
    "            'subject_predicates': self._predicate_encoder.transform_sequence(self.dataset['subject_predicates'][inds]),\n",
    "            'subject_objects': self._entity_encoder.transform_sequence(self.dataset['subject_objects'][inds]),\n",
    "            'predicate': self._predicate_encoder.transform(self.dataset['predicate'][inds]), \n",
    "            'object': self._entity_encoder.transform(self.dataset['object'][inds]),\n",
    "            'object_text': tokenize_sequence(self.dataset['object_text'][inds]),\n",
    "            'object_predicates': self._predicate_encoder.transform_sequence(self.dataset['object_predicates'][inds]),\n",
    "            'object_objects': self._entity_encoder.transform_sequence(self.dataset['object_objects'][inds]),\n",
    "            'other_subject': self._entity_encoder.transform(self.dataset['other_subject'][inds]),\n",
    "            'other_predicate': self._predicate_encoder.transform(self.dataset['other_predicate'][inds]), \n",
    "            'other_object': self._entity_encoder.transform(self.dataset['other_object'][inds]),\n",
    "            'other_object_text': tokenize_sequence(self.dataset['other_object_text'][inds]),\n",
    "            'other_entity_predicates': self._predicate_encoder.transform_sequence(self.dataset['other_entity_predicates'][inds]),\n",
    "            'other_entity_objects': self._entity_encoder.transform_sequence(self.dataset['other_entity_objects'][inds])\n",
    "        }, {\n",
    "            'add_subject': self._output_entity_encoder.transform(self.dataset['add_subject'][inds]),\n",
    "            'add_predicate': self._output_predicate_encoder.transform(self.dataset['add_predicate'][inds]), \n",
    "            'add_object': self._output_entity_encoder.transform(self.dataset['add_object'][inds]),\n",
    "            'del_subject': self._output_entity_encoder.transform(self.dataset['del_subject'][inds]),\n",
    "            'del_predicate': self._output_predicate_encoder.transform(self.dataset['del_predicate'][inds]),\n",
    "            'del_object': self._output_entity_encoder.transform(self.dataset['del_object'][inds])\n",
    "        }\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        gc.collect()\n",
    "        if self._shuffle:\n",
    "            print('shuffling dataset')\n",
    "            np.random.shuffle(self.indices)\n",
    "\n",
    "class EntityFactsEmbedding:\n",
    "    def __init__(self, entities_count: int, predicates_count: int, dropout: keras.layers.Dropout):\n",
    "        self._dropout = dropout\n",
    "        self._entity_predicates_embedding = keras.layers.Embedding(predicates_count, 128, mask_zero=True, name=\"predicate_embedding\")\n",
    "        self._entity_objects_embedding = keras.layers.Embedding(entities_count, 128, mask_zero=True, name=\"entity_embedding\")\n",
    "        self._entity_desc_combination = keras.layers.Concatenate(-1, name=\"entity_desc_combination\")\n",
    "        self._entity_desc_featurizer = keras.layers.Dense(128, activation='relu', name=\"entity_desc_featurizer\")\n",
    "        self._entity_desc_attention = tf.keras.layers.Attention(name=\"entity_desc_attention\")\n",
    "        self.to_sequence = keras.layers.Reshape((1,128), name=\"entity_desc_to_sequence\")\n",
    "        self._from_sequence = keras.layers.Reshape((128,), name=\"entity_desc_from_sequence\")\n",
    "        self._query_featurizer = keras.layers.Dense(128, activation='relu', name=\"entity_desc_query_featurizer\")\n",
    "        self._entity_desc_pool = keras.layers.GlobalMaxPooling1D(name=\"entity_desc_reduction\")\n",
    "\n",
    "    def __call__(self, predicates, objects, query = None):\n",
    "        value = self._entity_desc_featurizer(\n",
    "            self._dropout(\n",
    "                self._entity_desc_combination([\n",
    "                    self._entity_predicates_embedding(predicates),\n",
    "                    self._entity_objects_embedding(objects),\n",
    "                ])\n",
    "            )\n",
    "        )\n",
    "        if query is None:\n",
    "            return self._entity_desc_pool(value)\n",
    "        else:\n",
    "            return self._from_sequence(self._entity_desc_attention(\n",
    "                [self.to_sequence(self._query_featurizer(query)), value],\n",
    "            ))\n",
    "\n",
    "\n",
    "class Model:\n",
    "    def __init__(self, train_dataset: Dict[str,np.array], dev_dataset: Dict[str,np.array], epochs: int = 2,  batch_size: int = 32, dropout: float = 0., with_attention: bool = False, with_entity_facts: bool = True, with_literals: bool = True, with_constraint_id: bool = False, with_subject: bool = False):\n",
    "        embedding_size = 128\n",
    "        min_occurences = 100\n",
    "        dropout_layer = keras.layers.Dropout(dropout, name=\"dropout\")\n",
    "        \n",
    "        self._entity_encoder = TermEncoder(min_occurences, SEQUENCE_SIZE)\n",
    "        self._entity_encoder.fit_sequence(train_dataset['constraint_objects'])\n",
    "        self._entity_encoder.fit(train_dataset['subject'])\n",
    "        self._entity_encoder.fit(train_dataset['object'])\n",
    "        self._entity_encoder.fit(train_dataset['other_subject'])\n",
    "        self._entity_encoder.fit(train_dataset['other_object'])\n",
    "        self._entity_encoder.fit_sequence(train_dataset['subject_objects'])\n",
    "        self._entity_encoder.fit_sequence(train_dataset['object_objects'])\n",
    "        self._entity_encoder.fit_sequence(train_dataset['other_entity_objects'])\n",
    "        \n",
    "        self._output_entity_encoder = TermEncoder(min_occurences, SEQUENCE_SIZE)\n",
    "        self._output_entity_encoder.fit(train_dataset['add_subject'])\n",
    "        self._output_entity_encoder.fit(train_dataset['add_object'])\n",
    "        self._output_entity_encoder.fit(train_dataset['del_subject'])\n",
    "        self._output_entity_encoder.fit(train_dataset['del_object'])\n",
    "\n",
    "        self._predicate_encoder = TermEncoder(min_occurences, SEQUENCE_SIZE)\n",
    "        self._predicate_encoder.fit_sequence(train_dataset['constraint_predicates'])\n",
    "        self._predicate_encoder.fit(train_dataset['predicate'])\n",
    "        self._predicate_encoder.fit(train_dataset['other_predicate'])\n",
    "        self._predicate_encoder.fit_sequence(train_dataset['subject_predicates'])\n",
    "        self._predicate_encoder.fit_sequence(train_dataset['object_predicates'])\n",
    "        self._predicate_encoder.fit_sequence(train_dataset['other_entity_predicates'])\n",
    "        \n",
    "        self._output_predicate_encoder = TermEncoder(min_occurences, SEQUENCE_SIZE)\n",
    "        self._output_predicate_encoder.fit(train_dataset['add_predicate'])\n",
    "        self._output_predicate_encoder.fit(train_dataset['del_predicate'])\n",
    "        \n",
    "        self._constraint_id_encoder = TermEncoder(min_occurences, SEQUENCE_SIZE)\n",
    "        self._constraint_id_encoder.fit(train_dataset['constraint_id'])\n",
    "\n",
    "        print('Dataset stats: {} input predicates, {} input entities, {} output predicates, {} output entities'.format(len(self._predicate_encoder), len(self._entity_encoder), len(self._output_predicate_encoder), len(self._output_entity_encoder)))\n",
    "\n",
    "        word_embedding = keras.layers.Embedding(30000, embedding_size, mask_zero=True, name=\"text_embedding\")\n",
    "        text_pool = keras.layers.GlobalMaxPool1D(name=\"text_pool\")\n",
    "        #text_pool = keras.layers.Bidirectional(keras.layers.LSTM(128, name=\"text_lstm\"), name=\"text_pool\")\n",
    "        text_embedding = lambda i: text_pool(word_embedding(i))\n",
    "        entity_desc_embedding = EntityFactsEmbedding(len(self._entity_encoder), len(self._predicate_encoder), dropout_layer)\n",
    "        from_seq = keras.layers.Reshape((embedding_size,), name=\"from_sequence\")\n",
    "        predicate_embedding = lambda i: from_seq(entity_desc_embedding._entity_predicates_embedding(i))\n",
    "        entity_embedding = lambda i: from_seq(entity_desc_embedding._entity_objects_embedding(i))\n",
    "\n",
    "        # constraint\n",
    "        constraint_id_input = keras.Input(shape=(1,), name=\"constraint_id\")\n",
    "        constraint_predicates_input = keras.Input(shape=(None,), name=\"constraint_predicates\")\n",
    "        constraint_objects_input = keras.Input(shape=(None,), name=\"constraint_objects\")\n",
    "        if with_constraint_id:\n",
    "            constraint_id_embedding = keras.layers.Embedding(len(self._constraint_id_encoder), embedding_size, name=\"constraint_id_embedding\")\n",
    "            constraint_features = from_seq(constraint_id_embedding(constraint_id_input))\n",
    "        else:\n",
    "            constraint_features = entity_desc_embedding(predicates=constraint_predicates_input, objects=constraint_objects_input)\n",
    "\n",
    "        # violation\n",
    "        subject_input = keras.Input(shape=(1,), name=\"subject\")\n",
    "        subject_predicates_input = keras.Input(shape=(None,), name=\"subject_predicates\")\n",
    "        subject_objects_input = keras.Input(shape=(None,), name=\"subject_objects\")\n",
    "        predicate_input = keras.Input(shape=(1,), name=\"predicate\")\n",
    "        object_input = keras.Input(shape=(1,), name=\"object\")\n",
    "        object_text_input = keras.Input(shape=(None,), name=\"object_text\")\n",
    "        object_predicates_input = keras.Input(shape=(None,), name=\"object_predicates\")\n",
    "        object_objects_input = keras.Input(shape=(None,), name=\"object_objects\")\n",
    "        other_subject_input = keras.Input(shape=(1,), name=\"other_subject\")\n",
    "        other_predicate_input = keras.Input(shape=(1,), name=\"other_predicate\")\n",
    "        other_object_input = keras.Input(shape=(1,), name=\"other_object\")\n",
    "        other_object_text_input = keras.Input(shape=(None,), name=\"other_object_text\")\n",
    "        other_entity_predicates_input = keras.Input(shape=(None,), name=\"other_entity_predicates\")\n",
    "        other_entity_objects_input = keras.Input(shape=(None,), name=\"other_entity_objects\")\n",
    "\n",
    "        subject_features = entity_embedding(subject_input)\n",
    "        predicate_features = predicate_embedding(predicate_input)\n",
    "        object_features = entity_embedding(object_input)\n",
    "        object_text_features = text_embedding(object_text_input)\n",
    "        other_subject_features = entity_embedding(other_subject_input)\n",
    "        other_predicate_features = predicate_embedding(other_predicate_input)\n",
    "        other_object_features = entity_embedding(other_object_input)\n",
    "        other_object_text_features = text_embedding(other_object_text_input)\n",
    "        if with_attention:\n",
    "            subject_query_features = keras.layers.Dense(units=embedding_size, name=\"subject_query_features\")(dropout_layer(constraint_features))\n",
    "            subject_desc_features = entity_desc_embedding(predicates=subject_predicates_input, objects=subject_objects_input, query=subject_query_features)\n",
    "        else:\n",
    "            subject_desc_features = entity_desc_embedding(predicates=subject_predicates_input, objects=subject_objects_input)\n",
    "        if with_attention:\n",
    "            object_query_features = keras.layers.Dense(units=embedding_size, name=\"object_query_features\")(dropout_layer(constraint_features))\n",
    "            object_desc_features = entity_desc_embedding(predicates=object_predicates_input, objects=object_objects_input, query=object_query_features)\n",
    "        else:\n",
    "            object_desc_features = entity_desc_embedding(predicates=object_predicates_input, objects=object_objects_input)\n",
    "        if with_attention:\n",
    "            other_entity_query_features = keras.layers.Dense(units=embedding_size, name=\"other_entity_query_features\")(dropout_layer(constraint_features))\n",
    "            other_entity_desc_features = entity_desc_embedding(predicates=other_entity_predicates_input, objects=other_entity_objects_input, query=other_entity_query_features)\n",
    "        else:\n",
    "            other_entity_desc_features = entity_desc_embedding(predicates=other_entity_predicates_input, objects=other_entity_objects_input)\n",
    "\n",
    "        inputs = [\n",
    "            constraint_features,\n",
    "            predicate_features,\n",
    "            object_features, \n",
    "            other_predicate_features,\n",
    "            other_object_features\n",
    "        ]\n",
    "        if with_subject:\n",
    "            inputs.append(subject_features)\n",
    "            inputs.append(other_subject_features)\n",
    "        if with_entity_facts:\n",
    "            inputs.append(subject_desc_features)\n",
    "            inputs.append(object_desc_features)\n",
    "            inputs.append(other_entity_desc_features)\n",
    "        if with_literals:\n",
    "            inputs.append(object_text_features)\n",
    "            inputs.append(other_object_text_features)\n",
    "        dense_input = dropout_layer(keras.layers.concatenate(inputs, name=\"input_concat\"))\n",
    "        dense_l1 = dropout_layer(keras.layers.Dense(units=embedding_size*4, activation='relu', name=\"dense_1\")(dense_input))\n",
    "        dense_l2 = dropout_layer(keras.layers.Dense(units=embedding_size*4, activation='relu', name=\"dense_2\")(dense_l1))\n",
    "        add_subject = keras.layers.Dense(units=len(self._output_entity_encoder), activation='softmax', name=\"add_subject\")(dense_l2)\n",
    "        add_predicate = keras.layers.Dense(units=len(self._output_predicate_encoder), activation='softmax', name=\"add_predicate\")(dense_l2)\n",
    "        add_object = keras.layers.Dense(units=len(self._output_entity_encoder), activation='softmax', name=\"add_object\")(dense_l2)\n",
    "        del_subject = keras.layers.Dense(units=len(self._output_entity_encoder), activation='softmax', name=\"del_subject\")(dense_l2)\n",
    "        del_predicate = keras.layers.Dense(units=len(self._output_predicate_encoder), activation='softmax', name=\"del_predicate\")(dense_l2)\n",
    "        del_object = keras.layers.Dense(units=len(self._output_entity_encoder), activation='softmax', name=\"del_object\")(dense_l2)\n",
    "\n",
    "        self._model = keras.Model(inputs=[\n",
    "            constraint_id_input, constraint_predicates_input,  constraint_objects_input,\n",
    "            subject_input,\n",
    "            subject_predicates_input, subject_objects_input,\n",
    "            predicate_input,\n",
    "            object_input, object_text_input,\n",
    "            object_predicates_input, object_objects_input,\n",
    "            other_subject_input,\n",
    "            other_predicate_input,\n",
    "            other_object_input, other_object_text_input,\n",
    "            other_entity_predicates_input, other_entity_objects_input\n",
    "        ], outputs=[add_subject, add_predicate, add_object, del_subject, del_predicate, del_object])\n",
    "        self._model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['sparse_categorical_accuracy'])\n",
    "        print('model build done')\n",
    "        print(self._model.summary())\n",
    "\n",
    "        print('training with {} epochs, a batch size of {} and a dropout rate of {}'.format(epochs, batch_size, dropout))\n",
    "        \n",
    "        best_weights_filepath = './best_weights_corrections.hdf5'\n",
    "        early_stopping = keras.callbacks.EarlyStopping(monitor='val_loss', patience=1, verbose=1, mode='auto')\n",
    "        save_best_model = keras.callbacks.ModelCheckpoint(best_weights_filepath, monitor='val_loss', verbose=1, save_best_only=True, mode='auto')\n",
    "        history = self._model.fit(self._to_dataset(train_dataset, batch_size=batch_size, shuffle=True), epochs=epochs, validation_data=self._to_dataset(dev_dataset, batch_size=batch_size), callbacks=[early_stopping, save_best_model])\n",
    "        self._model.load_weights(best_weights_filepath)\n",
    "\n",
    "    def _to_dataset(self, dataset: Dict[str,np.array], batch_size, shuffle: bool = False):\n",
    "        return DatasetSequence(dataset, self._constraint_id_encoder, self._predicate_encoder, self._entity_encoder, self._output_predicate_encoder, self._output_entity_encoder, batch_size=batch_size, shuffle=shuffle)\n",
    "\n",
    "    def eval(self, dataset: Dict[str,np.array]):\n",
    "        ok_by_constraint = defaultdict(int)\n",
    "        error_by_constraint = defaultdict(int)\n",
    "        total_by_constraint = defaultdict(int)\n",
    "        ok = 0\n",
    "        error = 0\n",
    "        total = 0\n",
    "        parameters_found_and_correct = defaultdict(int)\n",
    "        parameters_predicted = defaultdict(int)\n",
    "        parameters_expected = defaultdict(int)\n",
    "\n",
    "        for add_subject_pred, add_predicate_pred, add_object_pred, del_subject_pred, del_predicate_pred, del_object_pred in model.predict(dataset):\n",
    "            constraint = dataset['constraint_id'][total]\n",
    "            predictions = {\n",
    "                'add_subject': add_subject_pred,\n",
    "                'add_predicate': add_predicate_pred,\n",
    "                'add_object': add_object_pred,\n",
    "                'del_subject': del_subject_pred,\n",
    "                'del_predicate': del_predicate_pred,\n",
    "                'del_object': del_object_pred,\n",
    "            }\n",
    "            if predictions['add_subject'] != 0 or predictions['add_predicate'] != 0 or predictions['add_object'] != 0:\n",
    "                if predictions['add_subject'] == 0 or predictions['add_predicate'] == 0 or predictions['add_object'] == 0:\n",
    "                    total += 1\n",
    "                    total_by_constraint[constraint] += 1\n",
    "                    continue # Missing value\n",
    "            if predictions['del_subject'] != 0 or predictions['del_predicate'] != 0 or predictions['del_object'] != 0:\n",
    "                if predictions['del_subject'] == 0 or predictions['del_predicate'] == 0 or predictions['del_object'] == 0:\n",
    "                    total += 1\n",
    "                    total_by_constraint[constraint] += 1\n",
    "                    continue # Missing value\n",
    " \n",
    "            if all(dataset[k][total] == v for k, v in predictions.items()):\n",
    "                ok += 1\n",
    "                ok_by_constraint[constraint] += 1\n",
    "            else:\n",
    "                error += 1\n",
    "                error_by_constraint[constraint] += 1\n",
    "            for k, v in predictions.items():\n",
    "                if v is not None:\n",
    "                    parameters_predicted[k] += 1\n",
    "                    if v == dataset[k][total] :\n",
    "                        parameters_found_and_correct[k] += 1\n",
    "                if dataset[k][total] is not None:\n",
    "                    parameters_expected[k] += 1\n",
    "            total_by_constraint[constraint] += 1\n",
    "            total += 1\n",
    "        \n",
    "        by_constraint = [self._precision_recall(ok_by_constraint[c], ok_by_constraint[c] + error_by_constraint[c], total_by_constraint[c]) for c in total_by_constraint.keys()]\n",
    "        return {\n",
    "            **self._precision_recall(ok, ok+error, total), \n",
    "            'accuracy': ok/total,\n",
    "            'parameters': {k: self._precision_recall(parameters_found_and_correct[k], parameters_predicted[k],v) for k,v in parameters_expected.items()},\n",
    "            'by_constraint': by_constraint,\n",
    "            'ok': ok,\n",
    "            'error': error,\n",
    "            'total': total\n",
    "        }\n",
    "    \n",
    "    @staticmethod\n",
    "    def _precision_recall(found_and_correct: int, predicted: int, expected: int) -> dict:\n",
    "        precision = found_and_correct / predicted if predicted else float('nan')\n",
    "        recall = found_and_correct / expected if expected else float('nan') # TODO: should be found and correct ??? This seems badly wrong and it's what we did in the CorHist paper\n",
    "        F1 = 2 * precision*recall / (precision+recall) if precision + recall else float('nan')\n",
    "        return {\n",
    "            'precision': precision,\n",
    "            'recall': recall,\n",
    "            'F1': F1\n",
    "        }\n",
    "\n",
    "    def predict(self, dataset: Dict[str,np.array]):\n",
    "        dataset = self._to_dataset(dataset, batch_size=128)\n",
    "        for i in range(len(dataset)):\n",
    "            add_subject_pred, add_predicate_pred, add_object_pred, del_subject_pred, del_predicate_pred, del_object_pred = self._model.predict(dataset[i][0])\n",
    "            add_subject_pred = self._output_entity_encoder.decode(np.argmax(add_subject_pred, 1))\n",
    "            add_predicate_pred = self._output_predicate_encoder.decode(np.argmax(add_predicate_pred, 1))\n",
    "            add_object_pred = self._output_entity_encoder.decode(np.argmax(add_object_pred, 1))\n",
    "            del_subject_pred = self._output_entity_encoder.decode(np.argmax(del_subject_pred, 1))\n",
    "            del_predicate_pred = self._output_predicate_encoder.decode(np.argmax(del_predicate_pred, 1))\n",
    "            del_object_pred = self._output_entity_encoder.decode(np.argmax(del_object_pred, 1))\n",
    "            yield from zip(add_subject_pred, add_predicate_pred, add_object_pred, del_subject_pred, del_predicate_pred, del_object_pred)\n",
    "    \n",
    "    def save(self, dir: str):\n",
    "        os.makedirs(dir, exist_ok=True)\n",
    "        self._entity_encoder.save(dir + '/entity_encoding.txt')\n",
    "        self._output_entity_encoder.save(dir + '/output_entity_encoding.txt')\n",
    "        self._predicate_encoder.save(dir + '/predicate_encoding.txt')\n",
    "        self._output_predicate_encoder.save(dir + '/output_predicate_encoding.txt')\n",
    "        self._constraint_id_encoder.save(dir + '/constraint_id_encoding.txt')\n",
    "        self._model.save(dir + '/model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset stats: 238 input predicates, 330 input entities, 6 output predicates, 8 output entities\n",
      "model build done\n",
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "constraint_predicates (InputLay [(None, None)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "constraint_objects (InputLayer) [(None, None)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "predicate (InputLayer)          [(None, 1)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "object (InputLayer)             [(None, 1)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "other_predicate (InputLayer)    [(None, 1)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "other_object (InputLayer)       [(None, 1)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "subject_predicates (InputLayer) [(None, None)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "subject_objects (InputLayer)    [(None, None)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "object_predicates (InputLayer)  [(None, None)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "object_objects (InputLayer)     [(None, None)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "other_entity_predicates (InputL [(None, None)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "other_entity_objects (InputLaye [(None, None)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "predicate_embedding (Embedding) multiple             30464       constraint_predicates[0][0]      \n",
      "                                                                 predicate[0][0]                  \n",
      "                                                                 other_predicate[0][0]            \n",
      "                                                                 subject_predicates[0][0]         \n",
      "                                                                 object_predicates[0][0]          \n",
      "                                                                 other_entity_predicates[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "entity_embedding (Embedding)    multiple             42240       constraint_objects[0][0]         \n",
      "                                                                 object[0][0]                     \n",
      "                                                                 other_object[0][0]               \n",
      "                                                                 subject_objects[0][0]            \n",
      "                                                                 object_objects[0][0]             \n",
      "                                                                 other_entity_objects[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "entity_desc_combination (Concat (None, None, 256)    0           predicate_embedding[0][0]        \n",
      "                                                                 entity_embedding[0][0]           \n",
      "                                                                 predicate_embedding[3][0]        \n",
      "                                                                 entity_embedding[5][0]           \n",
      "                                                                 predicate_embedding[4][0]        \n",
      "                                                                 entity_embedding[6][0]           \n",
      "                                                                 predicate_embedding[5][0]        \n",
      "                                                                 entity_embedding[7][0]           \n",
      "__________________________________________________________________________________________________\n",
      "dropout (Dropout)               multiple             0           entity_desc_combination[0][0]    \n",
      "                                                                 entity_desc_combination[1][0]    \n",
      "                                                                 entity_desc_combination[2][0]    \n",
      "                                                                 entity_desc_combination[3][0]    \n",
      "                                                                 input_concat[0][0]               \n",
      "                                                                 dense_1[0][0]                    \n",
      "                                                                 dense_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "object_text (InputLayer)        [(None, None)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "other_object_text (InputLayer)  [(None, None)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "entity_desc_featurizer (Dense)  (None, None, 128)    32896       dropout[0][0]                    \n",
      "                                                                 dropout[1][0]                    \n",
      "                                                                 dropout[2][0]                    \n",
      "                                                                 dropout[3][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "text_embedding (Embedding)      (None, None, 128)    3840000     object_text[0][0]                \n",
      "                                                                 other_object_text[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "entity_desc_reduction (GlobalMa (None, 128)          0           entity_desc_featurizer[0][0]     \n",
      "                                                                 entity_desc_featurizer[1][0]     \n",
      "                                                                 entity_desc_featurizer[2][0]     \n",
      "                                                                 entity_desc_featurizer[3][0]     \n",
      "__________________________________________________________________________________________________\n",
      "from_sequence (Reshape)         (None, 128)          0           predicate_embedding[1][0]        \n",
      "                                                                 entity_embedding[2][0]           \n",
      "                                                                 predicate_embedding[2][0]        \n",
      "                                                                 entity_embedding[4][0]           \n",
      "__________________________________________________________________________________________________\n",
      "text_pool (GlobalMaxPooling1D)  (None, 128)          0           text_embedding[0][0]             \n",
      "                                                                 text_embedding[1][0]             \n",
      "__________________________________________________________________________________________________\n",
      "input_concat (Concatenate)      (None, 1280)         0           entity_desc_reduction[0][0]      \n",
      "                                                                 from_sequence[1][0]              \n",
      "                                                                 from_sequence[2][0]              \n",
      "                                                                 from_sequence[4][0]              \n",
      "                                                                 from_sequence[5][0]              \n",
      "                                                                 entity_desc_reduction[1][0]      \n",
      "                                                                 entity_desc_reduction[2][0]      \n",
      "                                                                 entity_desc_reduction[3][0]      \n",
      "                                                                 text_pool[0][0]                  \n",
      "                                                                 text_pool[1][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 512)          655872      dropout[4][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 512)          262656      dropout[5][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "constraint_id (InputLayer)      [(None, 1)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "subject (InputLayer)            [(None, 1)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "other_subject (InputLayer)      [(None, 1)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "add_subject (Dense)             (None, 8)            4104        dropout[6][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "add_predicate (Dense)           (None, 6)            3078        dropout[6][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "add_object (Dense)              (None, 8)            4104        dropout[6][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "del_subject (Dense)             (None, 8)            4104        dropout[6][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "del_predicate (Dense)           (None, 6)            3078        dropout[6][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "del_object (Dense)              (None, 8)            4104        dropout[6][0]                    \n",
      "==================================================================================================\n",
      "Total params: 4,886,700\n",
      "Trainable params: 4,886,700\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n",
      "training with 20 epochs, a batch size of 256 and a dropout rate of 0.1\n",
      "shuffling dataset\n",
      "Epoch 1/20\n",
      "17/17 [==============================] - ETA: 0s - loss: 7.1628 - add_subject_loss: 1.2952 - add_predicate_loss: 1.3670 - add_object_loss: 1.1915 - del_subject_loss: 1.0912 - del_predicate_loss: 1.0388 - del_object_loss: 1.1791 - add_subject_sparse_categorical_accuracy: 0.4559 - add_predicate_sparse_categorical_accuracy: 0.4899 - add_object_sparse_categorical_accuracy: 0.7332 - del_subject_sparse_categorical_accuracy: 0.5744 - del_predicate_sparse_categorical_accuracy: 0.6073 - del_object_sparse_categorical_accuracy: 0.5696\n",
      "Epoch 00001: val_loss improved from inf to 4.74976, saving model to ./best_weights_corrections.hdf5\n",
      "17/17 [==============================] - 3s 174ms/step - loss: 7.1628 - add_subject_loss: 1.2952 - add_predicate_loss: 1.3670 - add_object_loss: 1.1915 - del_subject_loss: 1.0912 - del_predicate_loss: 1.0388 - del_object_loss: 1.1791 - add_subject_sparse_categorical_accuracy: 0.4559 - add_predicate_sparse_categorical_accuracy: 0.4899 - add_object_sparse_categorical_accuracy: 0.7332 - del_subject_sparse_categorical_accuracy: 0.5744 - del_predicate_sparse_categorical_accuracy: 0.6073 - del_object_sparse_categorical_accuracy: 0.5696 - val_loss: 4.7498 - val_add_subject_loss: 0.7827 - val_add_predicate_loss: 1.0850 - val_add_object_loss: 0.8201 - val_del_subject_loss: 0.6841 - val_del_predicate_loss: 0.6447 - val_del_object_loss: 0.7332 - val_add_subject_sparse_categorical_accuracy: 0.7679 - val_add_predicate_sparse_categorical_accuracy: 0.5354 - val_add_object_sparse_categorical_accuracy: 0.7856 - val_del_subject_sparse_categorical_accuracy: 0.7144 - val_del_predicate_sparse_categorical_accuracy: 0.7626 - val_del_object_sparse_categorical_accuracy: 0.7015\n",
      "shuffling dataset\n",
      "Epoch 2/20\n",
      "17/17 [==============================] - ETA: 0s - loss: 3.7661 - add_subject_loss: 0.6382 - add_predicate_loss: 0.9824 - add_object_loss: 0.7528 - del_subject_loss: 0.4086 - del_predicate_loss: 0.4575 - del_object_loss: 0.5267 - add_subject_sparse_categorical_accuracy: 0.7397 - add_predicate_sparse_categorical_accuracy: 0.6133 - add_object_sparse_categorical_accuracy: 0.7787 - del_subject_sparse_categorical_accuracy: 0.8511 - del_predicate_sparse_categorical_accuracy: 0.8415 - del_object_sparse_categorical_accuracy: 0.7815\n",
      "Epoch 00002: val_loss improved from 4.74976 to 3.10993, saving model to ./best_weights_corrections.hdf5\n",
      "17/17 [==============================] - 2s 120ms/step - loss: 3.7661 - add_subject_loss: 0.6382 - add_predicate_loss: 0.9824 - add_object_loss: 0.7528 - del_subject_loss: 0.4086 - del_predicate_loss: 0.4575 - del_object_loss: 0.5267 - add_subject_sparse_categorical_accuracy: 0.7397 - add_predicate_sparse_categorical_accuracy: 0.6133 - add_object_sparse_categorical_accuracy: 0.7787 - del_subject_sparse_categorical_accuracy: 0.8511 - del_predicate_sparse_categorical_accuracy: 0.8415 - del_object_sparse_categorical_accuracy: 0.7815 - val_loss: 3.1099 - val_add_subject_loss: 0.5605 - val_add_predicate_loss: 0.8314 - val_add_object_loss: 0.6379 - val_del_subject_loss: 0.3398 - val_del_predicate_loss: 0.3273 - val_del_object_loss: 0.4130 - val_add_subject_sparse_categorical_accuracy: 0.7603 - val_add_predicate_sparse_categorical_accuracy: 0.6781 - val_add_object_sparse_categorical_accuracy: 0.7856 - val_del_subject_sparse_categorical_accuracy: 0.8736 - val_del_predicate_sparse_categorical_accuracy: 0.8771 - val_del_object_sparse_categorical_accuracy: 0.7999\n",
      "shuffling dataset\n",
      "Epoch 3/20\n",
      "17/17 [==============================] - ETA: 0s - loss: 2.5938 - add_subject_loss: 0.4831 - add_predicate_loss: 0.6938 - add_object_loss: 0.5723 - del_subject_loss: 0.2130 - del_predicate_loss: 0.2711 - del_object_loss: 0.3604 - add_subject_sparse_categorical_accuracy: 0.8125 - add_predicate_sparse_categorical_accuracy: 0.7610 - add_object_sparse_categorical_accuracy: 0.8028 - del_subject_sparse_categorical_accuracy: 0.9389 - del_predicate_sparse_categorical_accuracy: 0.9072 - del_object_sparse_categorical_accuracy: 0.8548\n",
      "Epoch 00003: val_loss improved from 3.10993 to 2.42751, saving model to ./best_weights_corrections.hdf5\n",
      "17/17 [==============================] - 2s 125ms/step - loss: 2.5938 - add_subject_loss: 0.4831 - add_predicate_loss: 0.6938 - add_object_loss: 0.5723 - del_subject_loss: 0.2130 - del_predicate_loss: 0.2711 - del_object_loss: 0.3604 - add_subject_sparse_categorical_accuracy: 0.8125 - add_predicate_sparse_categorical_accuracy: 0.7610 - add_object_sparse_categorical_accuracy: 0.8028 - del_subject_sparse_categorical_accuracy: 0.9389 - del_predicate_sparse_categorical_accuracy: 0.9072 - del_object_sparse_categorical_accuracy: 0.8548 - val_loss: 2.4275 - val_add_subject_loss: 0.4331 - val_add_predicate_loss: 0.5778 - val_add_object_loss: 0.4610 - val_del_subject_loss: 0.3335 - val_del_predicate_loss: 0.2673 - val_del_object_loss: 0.3548 - val_add_subject_sparse_categorical_accuracy: 0.8267 - val_add_predicate_sparse_categorical_accuracy: 0.8166 - val_add_object_sparse_categorical_accuracy: 0.8784 - val_del_subject_sparse_categorical_accuracy: 0.8938 - val_del_predicate_sparse_categorical_accuracy: 0.9058 - val_del_object_sparse_categorical_accuracy: 0.8686\n",
      "shuffling dataset\n",
      "Epoch 4/20\n",
      "17/17 [==============================] - ETA: 0s - loss: 1.9608 - add_subject_loss: 0.3984 - add_predicate_loss: 0.5012 - add_object_loss: 0.3905 - del_subject_loss: 0.1628 - del_predicate_loss: 0.2034 - del_object_loss: 0.3045 - add_subject_sparse_categorical_accuracy: 0.8412 - add_predicate_sparse_categorical_accuracy: 0.8419 - add_object_sparse_categorical_accuracy: 0.8876 - del_subject_sparse_categorical_accuracy: 0.9497 - del_predicate_sparse_categorical_accuracy: 0.9320 - del_object_sparse_categorical_accuracy: 0.8702\n",
      "Epoch 00004: val_loss improved from 2.42751 to 2.12558, saving model to ./best_weights_corrections.hdf5\n",
      "17/17 [==============================] - 2s 108ms/step - loss: 1.9608 - add_subject_loss: 0.3984 - add_predicate_loss: 0.5012 - add_object_loss: 0.3905 - del_subject_loss: 0.1628 - del_predicate_loss: 0.2034 - del_object_loss: 0.3045 - add_subject_sparse_categorical_accuracy: 0.8412 - add_predicate_sparse_categorical_accuracy: 0.8419 - add_object_sparse_categorical_accuracy: 0.8876 - del_subject_sparse_categorical_accuracy: 0.9497 - del_predicate_sparse_categorical_accuracy: 0.9320 - del_object_sparse_categorical_accuracy: 0.8702 - val_loss: 2.1256 - val_add_subject_loss: 0.3803 - val_add_predicate_loss: 0.4915 - val_add_object_loss: 0.3429 - val_del_subject_loss: 0.3797 - val_del_predicate_loss: 0.2137 - val_del_object_loss: 0.3176 - val_add_subject_sparse_categorical_accuracy: 0.8603 - val_add_predicate_sparse_categorical_accuracy: 0.8676 - val_add_object_sparse_categorical_accuracy: 0.9067 - val_del_subject_sparse_categorical_accuracy: 0.8904 - val_del_predicate_sparse_categorical_accuracy: 0.9389 - val_del_object_sparse_categorical_accuracy: 0.8881\n",
      "shuffling dataset\n",
      "Epoch 5/20\n",
      "17/17 [==============================] - ETA: 0s - loss: 1.5989 - add_subject_loss: 0.3558 - add_predicate_loss: 0.3935 - add_object_loss: 0.2961 - del_subject_loss: 0.1337 - del_predicate_loss: 0.1565 - del_object_loss: 0.2633 - add_subject_sparse_categorical_accuracy: 0.8571 - add_predicate_sparse_categorical_accuracy: 0.8784 - add_object_sparse_categorical_accuracy: 0.9115 - del_subject_sparse_categorical_accuracy: 0.9557 - del_predicate_sparse_categorical_accuracy: 0.9485 - del_object_sparse_categorical_accuracy: 0.8888\n",
      "Epoch 00005: val_loss improved from 2.12558 to 1.72059, saving model to ./best_weights_corrections.hdf5\n",
      "17/17 [==============================] - 2s 128ms/step - loss: 1.5989 - add_subject_loss: 0.3558 - add_predicate_loss: 0.3935 - add_object_loss: 0.2961 - del_subject_loss: 0.1337 - del_predicate_loss: 0.1565 - del_object_loss: 0.2633 - add_subject_sparse_categorical_accuracy: 0.8571 - add_predicate_sparse_categorical_accuracy: 0.8784 - add_object_sparse_categorical_accuracy: 0.9115 - del_subject_sparse_categorical_accuracy: 0.9557 - del_predicate_sparse_categorical_accuracy: 0.9485 - del_object_sparse_categorical_accuracy: 0.8888 - val_loss: 1.7206 - val_add_subject_loss: 0.3427 - val_add_predicate_loss: 0.3692 - val_add_object_loss: 0.2817 - val_del_subject_loss: 0.3103 - val_del_predicate_loss: 0.1542 - val_del_object_loss: 0.2624 - val_add_subject_sparse_categorical_accuracy: 0.8672 - val_add_predicate_sparse_categorical_accuracy: 0.8711 - val_add_object_sparse_categorical_accuracy: 0.9164 - val_del_subject_sparse_categorical_accuracy: 0.8998 - val_del_predicate_sparse_categorical_accuracy: 0.9494 - val_del_object_sparse_categorical_accuracy: 0.8915\n",
      "shuffling dataset\n",
      "Epoch 6/20\n",
      "17/17 [==============================] - ETA: 0s - loss: 1.3459 - add_subject_loss: 0.3132 - add_predicate_loss: 0.3191 - add_object_loss: 0.2509 - del_subject_loss: 0.1098 - del_predicate_loss: 0.1258 - del_object_loss: 0.2271 - add_subject_sparse_categorical_accuracy: 0.8752 - add_predicate_sparse_categorical_accuracy: 0.8961 - add_object_sparse_categorical_accuracy: 0.9219 - del_subject_sparse_categorical_accuracy: 0.9623 - del_predicate_sparse_categorical_accuracy: 0.9575 - del_object_sparse_categorical_accuracy: 0.9040\n",
      "Epoch 00006: val_loss improved from 1.72059 to 1.65238, saving model to ./best_weights_corrections.hdf5\n",
      "17/17 [==============================] - 2s 119ms/step - loss: 1.3459 - add_subject_loss: 0.3132 - add_predicate_loss: 0.3191 - add_object_loss: 0.2509 - del_subject_loss: 0.1098 - del_predicate_loss: 0.1258 - del_object_loss: 0.2271 - add_subject_sparse_categorical_accuracy: 0.8752 - add_predicate_sparse_categorical_accuracy: 0.8961 - add_object_sparse_categorical_accuracy: 0.9219 - del_subject_sparse_categorical_accuracy: 0.9623 - del_predicate_sparse_categorical_accuracy: 0.9575 - del_object_sparse_categorical_accuracy: 0.9040 - val_loss: 1.6524 - val_add_subject_loss: 0.3463 - val_add_predicate_loss: 0.3303 - val_add_object_loss: 0.2528 - val_del_subject_loss: 0.3089 - val_del_predicate_loss: 0.1559 - val_del_object_loss: 0.2583 - val_add_subject_sparse_categorical_accuracy: 0.8796 - val_add_predicate_sparse_categorical_accuracy: 0.8892 - val_add_object_sparse_categorical_accuracy: 0.9267 - val_del_subject_sparse_categorical_accuracy: 0.9007 - val_del_predicate_sparse_categorical_accuracy: 0.9524 - val_del_object_sparse_categorical_accuracy: 0.8932\n",
      "shuffling dataset\n",
      "Epoch 7/20\n",
      "17/17 [==============================] - ETA: 0s - loss: 1.1983 - add_subject_loss: 0.2850 - add_predicate_loss: 0.2762 - add_object_loss: 0.2172 - del_subject_loss: 0.1007 - del_predicate_loss: 0.1129 - del_object_loss: 0.2063 - add_subject_sparse_categorical_accuracy: 0.8915 - add_predicate_sparse_categorical_accuracy: 0.9113 - add_object_sparse_categorical_accuracy: 0.9301 - del_subject_sparse_categorical_accuracy: 0.9644 - del_predicate_sparse_categorical_accuracy: 0.9614 - del_object_sparse_categorical_accuracy: 0.9177\n",
      "Epoch 00007: val_loss improved from 1.65238 to 1.61515, saving model to ./best_weights_corrections.hdf5\n",
      "17/17 [==============================] - 2s 128ms/step - loss: 1.1983 - add_subject_loss: 0.2850 - add_predicate_loss: 0.2762 - add_object_loss: 0.2172 - del_subject_loss: 0.1007 - del_predicate_loss: 0.1129 - del_object_loss: 0.2063 - add_subject_sparse_categorical_accuracy: 0.8915 - add_predicate_sparse_categorical_accuracy: 0.9113 - add_object_sparse_categorical_accuracy: 0.9301 - del_subject_sparse_categorical_accuracy: 0.9644 - del_predicate_sparse_categorical_accuracy: 0.9614 - del_object_sparse_categorical_accuracy: 0.9177 - val_loss: 1.6152 - val_add_subject_loss: 0.3256 - val_add_predicate_loss: 0.2994 - val_add_object_loss: 0.2346 - val_del_subject_loss: 0.3185 - val_del_predicate_loss: 0.1643 - val_del_object_loss: 0.2728 - val_add_subject_sparse_categorical_accuracy: 0.8847 - val_add_predicate_sparse_categorical_accuracy: 0.9065 - val_add_object_sparse_categorical_accuracy: 0.9364 - val_del_subject_sparse_categorical_accuracy: 0.9010 - val_del_predicate_sparse_categorical_accuracy: 0.9517 - val_del_object_sparse_categorical_accuracy: 0.8872\n",
      "shuffling dataset\n",
      "Epoch 8/20\n",
      "17/17 [==============================] - ETA: 0s - loss: 1.0821 - add_subject_loss: 0.2600 - add_predicate_loss: 0.2391 - add_object_loss: 0.1921 - del_subject_loss: 0.0923 - del_predicate_loss: 0.1025 - del_object_loss: 0.1962 - add_subject_sparse_categorical_accuracy: 0.9000 - add_predicate_sparse_categorical_accuracy: 0.9242 - add_object_sparse_categorical_accuracy: 0.9377 - del_subject_sparse_categorical_accuracy: 0.9674 - del_predicate_sparse_categorical_accuracy: 0.9648 - del_object_sparse_categorical_accuracy: 0.9175\n",
      "Epoch 00008: val_loss improved from 1.61515 to 1.49522, saving model to ./best_weights_corrections.hdf5\n",
      "17/17 [==============================] - 2s 116ms/step - loss: 1.0821 - add_subject_loss: 0.2600 - add_predicate_loss: 0.2391 - add_object_loss: 0.1921 - del_subject_loss: 0.0923 - del_predicate_loss: 0.1025 - del_object_loss: 0.1962 - add_subject_sparse_categorical_accuracy: 0.9000 - add_predicate_sparse_categorical_accuracy: 0.9242 - add_object_sparse_categorical_accuracy: 0.9377 - del_subject_sparse_categorical_accuracy: 0.9674 - del_predicate_sparse_categorical_accuracy: 0.9648 - del_object_sparse_categorical_accuracy: 0.9175 - val_loss: 1.4952 - val_add_subject_loss: 0.3232 - val_add_predicate_loss: 0.2827 - val_add_object_loss: 0.2159 - val_del_subject_loss: 0.2903 - val_del_predicate_loss: 0.1417 - val_del_object_loss: 0.2414 - val_add_subject_sparse_categorical_accuracy: 0.8819 - val_add_predicate_sparse_categorical_accuracy: 0.9187 - val_add_object_sparse_categorical_accuracy: 0.9375 - val_del_subject_sparse_categorical_accuracy: 0.9076 - val_del_predicate_sparse_categorical_accuracy: 0.9552 - val_del_object_sparse_categorical_accuracy: 0.9021\n",
      "shuffling dataset\n",
      "Epoch 9/20\n",
      "17/17 [==============================] - ETA: 0s - loss: 0.9900 - add_subject_loss: 0.2434 - add_predicate_loss: 0.2214 - add_object_loss: 0.1738 - del_subject_loss: 0.0814 - del_predicate_loss: 0.0949 - del_object_loss: 0.1751 - add_subject_sparse_categorical_accuracy: 0.9076 - add_predicate_sparse_categorical_accuracy: 0.9258 - add_object_sparse_categorical_accuracy: 0.9442 - del_subject_sparse_categorical_accuracy: 0.9708 - del_predicate_sparse_categorical_accuracy: 0.9639 - del_object_sparse_categorical_accuracy: 0.9329\n",
      "Epoch 00009: val_loss improved from 1.49522 to 1.46274, saving model to ./best_weights_corrections.hdf5\n",
      "17/17 [==============================] - 2s 114ms/step - loss: 0.9900 - add_subject_loss: 0.2434 - add_predicate_loss: 0.2214 - add_object_loss: 0.1738 - del_subject_loss: 0.0814 - del_predicate_loss: 0.0949 - del_object_loss: 0.1751 - add_subject_sparse_categorical_accuracy: 0.9076 - add_predicate_sparse_categorical_accuracy: 0.9258 - add_object_sparse_categorical_accuracy: 0.9442 - del_subject_sparse_categorical_accuracy: 0.9708 - del_predicate_sparse_categorical_accuracy: 0.9639 - del_object_sparse_categorical_accuracy: 0.9329 - val_loss: 1.4627 - val_add_subject_loss: 0.3096 - val_add_predicate_loss: 0.2572 - val_add_object_loss: 0.2152 - val_del_subject_loss: 0.2823 - val_del_predicate_loss: 0.1481 - val_del_object_loss: 0.2504 - val_add_subject_sparse_categorical_accuracy: 0.8909 - val_add_predicate_sparse_categorical_accuracy: 0.9177 - val_add_object_sparse_categorical_accuracy: 0.9380 - val_del_subject_sparse_categorical_accuracy: 0.9062 - val_del_predicate_sparse_categorical_accuracy: 0.9563 - val_del_object_sparse_categorical_accuracy: 0.8991\n",
      "shuffling dataset\n",
      "Epoch 10/20\n",
      "17/17 [==============================] - ETA: 0s - loss: 0.8965 - add_subject_loss: 0.2195 - add_predicate_loss: 0.1944 - add_object_loss: 0.1602 - del_subject_loss: 0.0768 - del_predicate_loss: 0.0833 - del_object_loss: 0.1624 - add_subject_sparse_categorical_accuracy: 0.9157 - add_predicate_sparse_categorical_accuracy: 0.9370 - add_object_sparse_categorical_accuracy: 0.9462 - del_subject_sparse_categorical_accuracy: 0.9733 - del_predicate_sparse_categorical_accuracy: 0.9713 - del_object_sparse_categorical_accuracy: 0.9414\n",
      "Epoch 00010: val_loss improved from 1.46274 to 1.40147, saving model to ./best_weights_corrections.hdf5\n",
      "17/17 [==============================] - 2s 118ms/step - loss: 0.8965 - add_subject_loss: 0.2195 - add_predicate_loss: 0.1944 - add_object_loss: 0.1602 - del_subject_loss: 0.0768 - del_predicate_loss: 0.0833 - del_object_loss: 0.1624 - add_subject_sparse_categorical_accuracy: 0.9157 - add_predicate_sparse_categorical_accuracy: 0.9370 - add_object_sparse_categorical_accuracy: 0.9462 - del_subject_sparse_categorical_accuracy: 0.9733 - del_predicate_sparse_categorical_accuracy: 0.9713 - del_object_sparse_categorical_accuracy: 0.9414 - val_loss: 1.4015 - val_add_subject_loss: 0.2717 - val_add_predicate_loss: 0.2448 - val_add_object_loss: 0.1868 - val_del_subject_loss: 0.3097 - val_del_predicate_loss: 0.1406 - val_del_object_loss: 0.2479 - val_add_subject_sparse_categorical_accuracy: 0.9104 - val_add_predicate_sparse_categorical_accuracy: 0.9246 - val_add_object_sparse_categorical_accuracy: 0.9435 - val_del_subject_sparse_categorical_accuracy: 0.9079 - val_del_predicate_sparse_categorical_accuracy: 0.9561 - val_del_object_sparse_categorical_accuracy: 0.8994\n",
      "shuffling dataset\n",
      "Epoch 11/20\n",
      "17/17 [==============================] - ETA: 0s - loss: 0.8095 - add_subject_loss: 0.2006 - add_predicate_loss: 0.1769 - add_object_loss: 0.1509 - del_subject_loss: 0.0637 - del_predicate_loss: 0.0774 - del_object_loss: 0.1400 - add_subject_sparse_categorical_accuracy: 0.9210 - add_predicate_sparse_categorical_accuracy: 0.9409 - add_object_sparse_categorical_accuracy: 0.9478 - del_subject_sparse_categorical_accuracy: 0.9777 - del_predicate_sparse_categorical_accuracy: 0.9733 - del_object_sparse_categorical_accuracy: 0.9467\n",
      "Epoch 00011: val_loss did not improve from 1.40147\n",
      "17/17 [==============================] - 2s 101ms/step - loss: 0.8095 - add_subject_loss: 0.2006 - add_predicate_loss: 0.1769 - add_object_loss: 0.1509 - del_subject_loss: 0.0637 - del_predicate_loss: 0.0774 - del_object_loss: 0.1400 - add_subject_sparse_categorical_accuracy: 0.9210 - add_predicate_sparse_categorical_accuracy: 0.9409 - add_object_sparse_categorical_accuracy: 0.9478 - del_subject_sparse_categorical_accuracy: 0.9777 - del_predicate_sparse_categorical_accuracy: 0.9733 - del_object_sparse_categorical_accuracy: 0.9467 - val_loss: 1.4419 - val_add_subject_loss: 0.2771 - val_add_predicate_loss: 0.2544 - val_add_object_loss: 0.1891 - val_del_subject_loss: 0.2882 - val_del_predicate_loss: 0.1514 - val_del_object_loss: 0.2818 - val_add_subject_sparse_categorical_accuracy: 0.9113 - val_add_predicate_sparse_categorical_accuracy: 0.9262 - val_add_object_sparse_categorical_accuracy: 0.9444 - val_del_subject_sparse_categorical_accuracy: 0.9065 - val_del_predicate_sparse_categorical_accuracy: 0.9522 - val_del_object_sparse_categorical_accuracy: 0.8936\n",
      "Epoch 00011: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "49"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = None\n",
    "# hack to clear memory before creating the new model\n",
    "gc.collect()\n",
    "model = Model(train_dataset, dev_dataset, epochs=20, batch_size=256, dropout=0.1,\n",
    "              with_attention=False, with_entity_facts=True, with_literals=True, with_constraint_id=False, with_subject=False)\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* conflictWith: Precision: 0.7588424437299035 Recall: 0.6145833333333334 F1: 0.6791366906474819\n",
      "* distinct: Precision: 0.5338541666666666 Recall: 0.5338541666666666 F1: 0.5338541666666666\n",
      "* inverse: Precision: 0.9375 Recall: 0.2734375 F1: 0.42338709677419356\n",
      "* itemRequiresStatement: Precision: 0.8 Recall: 0.020833333333333332 F1: 0.04060913705583756\n",
      "* oneOf: Precision: 0.7937219730941704 Recall: 0.4609375 F1: 0.5831960461285008\n",
      "* single: Precision: 0.4812680115273775 Recall: 0.4348958333333333 F1: 0.45690834473324216\n",
      "* type: Precision: 0.96875 Recall: 0.16145833333333334 F1: 0.27678571428571436\n",
      "* valueRequiresStatement: Precision: 0.9252336448598131 Recall: 0.2578125 F1: 0.4032586558044806\n",
      "* valueType: Precision: 0.0 Recall: 0.0 F1: nan\n",
      "* Micro average: Precision: 0.6784112748238309 Recall: 0.3064236111111111 F1: 0.42216464022324096\n",
      "* Macro average by constraint: Precision: 0.5813051113078221 Recall: 0.21394759316412207 F1: 0.31277813645636776\n",
      "* Macro average by type: Precision: 0.68879669331977 Recall: 0.3064236111111111 F1: 0.42415446941495755\n"
     ]
    }
   ],
   "source": [
    "# We run the evaluation against the test set\n",
    "# Because we trained on less data, the performances are lower than on the full training dataset\n",
    "\n",
    "import math\n",
    "\n",
    "results = {}\n",
    "for target, ds in sorted(test_dataset.items()):\n",
    "    result = model.eval(ds)\n",
    "    gc.collect()\n",
    "    results[target] = result\n",
    "    print('* {}: Precision: {} Recall: {} F1: {}'.format(target, result['precision'], result['recall'], result['F1']))\n",
    "\n",
    "def aggregate_prec_recall_f1(elements):\n",
    "    filtered_prec = [v['precision'] for v in elements if not math.isnan(v['precision'])]\n",
    "    filtered_rec = [v['recall'] for v in elements if not math.isnan(v['recall'])]\n",
    "    precision = sum(filtered_prec) / len(filtered_prec)\n",
    "    recall = sum(filtered_rec) / len(filtered_rec)\n",
    "    return {\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'F1': 2 * precision*recall / (precision+recall)\n",
    "    }\n",
    "\n",
    "micro = {\n",
    "    'precision': sum(ds['ok'] for ds in results.values()) / sum(ds['ok'] + ds['error'] for ds in results.values()),\n",
    "    'recall': sum(ds['ok'] for ds in results.values()) / sum(ds['total'] for ds in results.values())\n",
    "}\n",
    "micro['F1'] = 2*micro['precision'] * micro['recall'] / (micro['precision'] + micro['recall'])\n",
    "macro_by_type = aggregate_prec_recall_f1(results.values())\n",
    "macro_by_constraint = aggregate_prec_recall_f1(list(chain.from_iterable(r['by_constraint'] for r in results.values())))\n",
    "print('* Micro average: Precision: {} Recall: {} F1: {}'.format(micro['precision'], micro['recall'], micro['F1']))\n",
    "print('* Macro average by constraint: Precision: {} Recall: {} F1: {}'.format(macro_by_constraint['precision'], macro_by_constraint['recall'], macro_by_constraint['F1']))\n",
    "print('* Macro average by type: Precision: {} Recall: {} F1: {}'.format(macro_by_type['precision'], macro_by_type['recall'], macro_by_type['F1']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
